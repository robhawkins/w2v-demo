{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    /* style for notebook and presentation */\n",
       "    a { color: green !important; }\n",
       "    /* style for presentation only */\n",
       "    .reveal a { color: green !important; }\n",
       "    .reveal pasfh { color: red; }\n",
       "</style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "    /* style for notebook and presentation */\n",
    "    a { color: green !important; }\n",
    "    /* style for presentation only */\n",
    "    .reveal a { color: green !important; }\n",
    "    .reveal pasfh { color: red; }\n",
    "</style> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* An algorithm that can learn the meanings of words without much effort from the developer (Rob)\n",
    "* 一種算法，可以從顯影劑學習單詞的含義不費力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* \"Efficient Estimation of Word Representations in Vector Space\" (Mikolov et al., 2013)\n",
    "* 字表示的向量空間有效估計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can show similar words - e.g. 衣服\n",
    "```\n",
    "鞋子 0.857314109802\n",
    "大衣 0.82208108902\n",
    "穿 0.801305949688\n",
    "穿戴 0.800143063068\n",
    "裙子 0.795694112778\n",
    "外套 0.79101806879\n",
    "衣物 0.786619484425\n",
    "```\n",
    "English: shoes, coat, wear, wear, skirts, jacket, clothing, coat, wearing, wear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why are clothes and shoes similar?  為什麼衣服和鞋相似？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Look at the nearby words:\n",
    " * He wears **clothes** to work.  In the morning he puts on **clothes**\n",
    " * He wears **shoes** to work.  In the morning he puts on **shoes**\n",
    "* 看看這些句子：\n",
    " * 他穿衣服上班。早上，他穿上衣服\n",
    " * 他穿鞋上班。早上，他穿上鞋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Word2Vec?  Why not something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In short: Word2Vec is better at this than previous algorithms.  More details: [Don't count, predict!](http://www.marekrei.com/blog/dont-count-predict/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gensim implementation is easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* word2vec是魔術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print gensim.models.word2vec.FAST_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Want FAST_VERSION >= 0.  -1 means your code will run much more slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download data\n",
    "\n",
    "All of Chinese Wikipedia:\n",
    "\n",
    "`wget https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "$ du -csh zhwiki-latest-pages-articles.xml.bz2 \n",
    "1.2G\tzhwiki-latest-pages-articles.xml.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next: Convert .xml.bz2 to plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# process_wiki.py\n",
    "# Source: http://textminingonline.com/tag/word2vec-python\n",
    "# 中文: http://www.52nlp.cn/中英文维基百科语料上的word2vec实验\n",
    "\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    " \n",
    "from gensim.corpora import WikiCorpus\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    " \n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    " \n",
    "    # check and process input arguments\n",
    "    if len(sys.argv) < 3:\n",
    "        print globals()['__doc__'] % locals()\n",
    "        sys.exit(1)\n",
    "    inp, outp = sys.argv[1:3]\n",
    "    space = \" \"\n",
    "    i = 0\n",
    " \n",
    "    output = open(outp, 'w')\n",
    "    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(space.join(text) + \"\\n\")\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            logger.info(\"Saved \" + str(i) + \" articles\")\n",
    " \n",
    "    output.close()\n",
    "    logger.info(\"Finished Saved \" + str(i) + \" articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Save and run it\n",
    "\n",
    "* python process_wiki.py zhwiki-latest-pages-articles.xml.bz2 wiki.zh.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "歐幾里得 西元前三世紀的希臘數學家 現在被認為是幾何之父 此畫為拉斐爾的作品 雅典學院 数学 mathematics 是利用符号语言研究數量 结构 变化以及空间等概念的一門学科 从某种角度看屬於形式科學的一種 數學透過抽象化和邏�"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -c300 wiki.zh.text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segment words with Jeiba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import jieba\n",
    "\n",
    "\n",
    "jieba.enable_parallel(4)\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "\n",
    "url = 'wiki.zh.text'\n",
    "t1 = time.time()\n",
    "\n",
    "with open(url+'.seg','wb') as w:\n",
    "  with open(url) as f:\n",
    "    for line in f:\n",
    "      words = \" \".join(jieba.cut(line))\n",
    "      w.write(words.encode('utf-8'))\n",
    "\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "歐幾里 得   西元前 三世 紀的 希臘 數學家   現在 被 認為 是 幾何 之父   此畫 為拉斐爾 的 作品   雅典 學院   数学   mathematics   是 利用 符号语言 研究 數量   结构   变化 以及 空间 等 概念 的 一門 学科   从 某种 角度看 屬 於 形式 科學 的 一種   數學 透過 抽象化 和 邏輯 推理 的 使用   由計數   計算   數學家們 拓展 這些 概念   對 數學 基本概念 的 完善   早 在 古埃及   而 �"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -c 500 wiki.zh.text.seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Good, more spaces!  It is okay if there are some mistakes.  We can still get useful results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A different word segmenter might give better results.  See [cws_evaluation](https://github.com/ysc/cws_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training - train_word2vec_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# train_word2vec_model.py\n",
    " \n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "im port multiprocessing\n",
    " \n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    " \n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "    # check and process input arguments\n",
    "    if len(sys.argv) < 3:\n",
    "        print globals()['__doc__'] % locals()\n",
    "        sys.exit(1)\n",
    "    inp, outp = sys.argv[1:3]\n",
    "    ## Each line in inp must contain < 10,000 tokens\n",
    "    model = Word2Vec(LineSentence(inp), size=150, window=7, min_count=20,\n",
    "            workers=multiprocessing.cpu_count(), iter=5)\n",
    " \n",
    "    # trim unneeded model memory = use(much) less RAM\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameters\n",
    "* Word2Vec(..., size, window, min_count, workers, iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* size: 150 -- Usually: 100-400 -- Number of dimensions\n",
    " * More is better, but takes more time to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* window: 7 -- Usually: 5-10 -- Number of words of context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* min_count: 20 -- Usually: 5-20 --  The number of times a word must appear in your data to be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* workers: Number of CPUs you have.\n",
    " * The more, the better.  Gensim's Word2Vec only works on one machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* iter: 5 -- Normal: 10-20 -- Default is 1.\n",
    " * Important for improving your results\n",
    " * A higher number will increase run time\n",
    " * Higher number will increase model quality, up to a point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "* python train_word2vec_model.py wiki.zh.text.seg wiki.zh.text.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Time to run: ~2 hours\n",
    " * 4-core 1.6GHz i5 macbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki.zh.text.model\r\n",
      "wiki.zh.text.model.syn0.npy\r\n",
      "wiki.zh.text.model.syn1.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls -1 wiki.zh.text.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'\\u978b\\u5b50', 0.8573141098022461), (u'\\u5927\\u8863', 0.8220810890197754), (u'\\u7a7f', 0.8013059496879578), (u'\\u7a7f\\u6234', 0.8001430630683899), (u'\\u88d9\\u5b50', 0.79569411277771), (u'\\u5916\\u5957', 0.7910180687904358), (u'\\u8863\\u7269', 0.7866194844245911), (u'\\u5916\\u8863', 0.7854279279708862), (u'\\u7a7f\\u7740', 0.7846043109893799), (u'\\u6234\\u4e0a', 0.7763027548789978)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "m = Word2Vec.load('wiki.zh.text.model')\n",
    "\n",
    "print m.most_similar(u'衣服')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鞋子 0.928656160831\n",
      "大衣 0.911039590836\n",
      "穿 0.900652050972\n",
      "穿戴 0.900070667267\n",
      "裙子 0.897846162319\n",
      "外套 0.895508170128\n",
      "衣物 0.893308877945\n",
      "外衣 0.892713069916\n",
      "穿着 0.892301261425\n",
      "戴上 0.888150513172\n"
     ]
    }
   ],
   "source": [
    "def sim(m,p,n=[],i=10):\n",
    "    res = m.most_similar_cosmul(positive=p,negative=n,topn=i)\n",
    "    for e in res:\n",
    "        print e[0],e[1]\n",
    "\n",
    "sim(m,u'衣服')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What else can the results show?\n",
    "* Paris, France has same **relationship** as Dublin, Ireland\n",
    " * X is the capital of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* France - Paris = Ireland - Dublin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can frame this as a question to Word2Vec:\n",
    "  * What is the answer to France - Paris + Dublin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "愛爾蘭 1.03317070007\n"
     ]
    }
   ],
   "source": [
    "# France - Paris + Dublin = Ireland\n",
    "sim(m,[u'法國',u'都柏林'],[u'巴黎'],i=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Think of each word as a variable, for example:\n",
    "  * France = 10; Paris = 50; Dublin = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 10 - 50 + 200 = 160\n",
    " * Word2Vec looks for the word whose value is nearest 160 (Ireland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Actually:\n",
    " * France = [0.8319, .9318, .0831, .1485, ... ]\n",
    " * Each word has a list of numbers called a **word vector** or **word embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台灣 0.947201609612\n"
     ]
    }
   ],
   "source": [
    "# England + Taipei - London = Taiwan\n",
    "sim(m,[u'英國',u'台北'],[u'倫敦'],i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不便 0.884090006351\n"
     ]
    }
   ],
   "source": [
    "# Unreasonable - reasonable + convenient = inconvenient\n",
    "sim(m,[u'不合理',u'方便'],[u'合理'],i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "倪齐民 0.880974709988\n",
      "玫 0.869373738766\n",
      "苗可丽 0.868878781796\n",
      "琄 0.864685177803\n",
      "王仲篪 0.861672520638\n",
      "王琦 0.86028701067\n",
      "李 0.859799146652\n",
      "祺 0.859715044498\n",
      "宏恩 0.858121335506\n",
      "劉金環 0.857797265053\n"
     ]
    }
   ],
   "source": [
    "## The classic example didn't work:  king - man + woman = queen\n",
    "\n",
    "sim(m,[u'王',u'女人'],[u'男人'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "登基 0.893643379211\n",
      "封号 0.890938282013\n",
      "忽必烈 0.889511108398\n",
      "尊号 0.888372242451\n",
      "封其为 0.88481169939\n",
      "雍正帝 0.883169054985\n",
      "册封 0.880160093307\n",
      "康熙皇帝 0.879712939262\n",
      "诸王 0.878517746925\n",
      "觐见 0.877376556396\n"
     ]
    }
   ],
   "source": [
    "# Also doesn't work:  Emperor + Woman - Man\n",
    "sim(m,[u'皇帝',u'女人'],[u'男人'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother 1.06075584888\n",
      "daughter 1.03906154633\n",
      "daughters 1.00900793076\n",
      "faith 1.00623333454\n",
      "our 1.00440311432\n",
      "child 1.00097632408\n",
      "almost 1.00089430809\n",
      "witness 1.00083208084\n",
      "himself 0.997282207012\n",
      "wife 0.992625772953\n"
     ]
    }
   ],
   "source": [
    "## Chinese wikipedia apparently has some english\n",
    "\n",
    "sim(m,['father','woman'],['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一家人 0.938520371914\n",
      "奶奶 0.908418834209\n",
      "老婆 0.904512822628\n",
      "老爸 0.901482105255\n",
      "老公 0.900539815426\n",
      "丈母娘 0.897194266319\n",
      "幸福 0.887105762959\n",
      "娘家 0.886565566063\n",
      "千金 0.883036851883\n",
      "老伴 0.882726192474\n"
     ]
    }
   ],
   "source": [
    "## One more example.  Might be improved with more data,\n",
    "##                    or adjusting training parameters.\n",
    "\n",
    "sim(m,[u'爸爸',u'女人'],[u'男人'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So what?  Why do analogies matter?\n",
    "* Unit testing: Test the quality of this part of a software pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Extensions: We can use these results to do other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Little work required: All we need is lots of raw text and CPU power to learn word meanings in any language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications\n",
    "* Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sentiment analysis, stock market prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* News category classification: {sports, politics, technology}?   What section is this text (title, introduction, main, conclusion)?  [Smartnews.com video](http://www.youtube.com/watch?v=7BgpaZltW8s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Guessing gender of blog author, Dato.com](http://blog.dato.com/practical-text-analysis-using-deep-learning)\n",
    " * Was this blog written by man or woman?  How old?\n",
    " * [Notebook found here](https://dato.com/learn/gallery/notebooks/deep_text_learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* [Gender studies, (Ben Schmidt)](http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html)\n",
    " * Discovers words used to describe men vs women\n",
    "  * cologne->perfume\n",
    "  * cool->sweet\n",
    " * Results for Chinese might be interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other implementations\n",
    "* Google's, the original: https://code.google.com/p/word2vec/\n",
    " * [Google News vectors](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing), 2 GB model, pre-trained, on 100 billion words.  Vocab size is 3 million words\n",
    "* Gensim: https://radimrehurek.com/gensim/models/word2vec.html\n",
    " * The fastest, 4x faster than Google's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* DeepLearning4Java: http://deeplearning4j.org/word2vec.html\n",
    "* Spark ML Lib: https://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec\n",
    " * Gensim creator [says there are problems with it](https://groups.google.com/forum/#!searchin/gensim/spark$20word2vec/gensim/hAU7syBUqdM/2f_VtVoWAwAJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Misc. projects on GitHub with less documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Related work and extensions\n",
    "* Word2Vec phrases: https://radimrehurek.com/gensim/models/phrases.html#module-gensim.models.phrases\n",
    " * Combines frequently co-occuring tokens like **New_York** and **Chicago_Bulls**\n",
    " * Then, run Word2Vec as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Paragraph Vector](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    " * Same as Word2Vec, but for sentences, paragraphs or documents\n",
    " * Gensim implementation is called [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    " * Example notebook on [classifying IMDB movie reviews](https://github.com/linanqiu/word2vec-sentiments/blob/master/word2vec-sentiment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Sense2vec: http://arxiv.org/abs/1511.06388\n",
    " * Combine grammar tags with words to get better word vectors\n",
    " * For example: take_VERB could be separate from take_NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing via Dimensionality Reduction - T-SNE\n",
    "<img src='tsne.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing via Dimensionality Reduction\n",
    "[3D Word vectors](https://www.a1k0n.net/spotify/artist-viz/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "* Word2Vec is a good choice for many applications, and often the first step in a Machine Learning NLP pipeline.\n",
    "* [Wikipedia example in Chinese](http://ww.52nlp.cn/中英文维基百科语料上的word2vec实验)\n",
    "* [Wikipedia example in English](http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim)\n",
    "* [Gensim google group](https://groups.google.com/forum/#!forum/gensim) for Q & A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What other data sources can you think of that have lots of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Anything you can crawl on the web - PTT, Twitter, Job postings, blogs, research publications, Guttenberg press"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The end.  Thank you!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
